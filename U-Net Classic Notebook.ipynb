{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7de00a98",
   "metadata": {},
   "source": [
    "# Modèle classique U-Net (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49bc149",
   "metadata": {},
   "source": [
    "## Installations, importations et configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f362f3a",
   "metadata": {},
   "source": [
    "### Installations modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7deb65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q albumentations\n",
    "!pip install monai --upgrade --no-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b6529",
   "metadata": {},
   "source": [
    "### Importations Libairies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2cf060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "#MONAI:\n",
    "from monai.transforms import Compose,RandFlipd,RandRotate90d,RandAffined\n",
    "from monai.transforms import RandGridDistortiond,RandAdjustContrastd\n",
    "from monai.transforms import NormalizeIntensityd,EnsureChannelFirstd,ToTensord\n",
    "\n",
    "# Traitement image:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from skimage.measure import label\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import nibabel as nib\n",
    "from collections import defaultdict\n",
    "\n",
    "# Visualisation:\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Files Management:\n",
    "import os\n",
    "import zipfile\n",
    "import wandb\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145793d0",
   "metadata": {},
   "source": [
    "### Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b190d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "# print(torch.__version__)\n",
    "\n",
    "## À ajuster selon votre environnement\n",
    "# Chemin vers le fichier zip dans Google Drive\n",
    "drive.mount('/content/drive')\n",
    "zip_path = \"/content/drive/MyDrive/dataset.zip\"  # chemin zip dans Drive\n",
    "extract_path = \"/content/dataset\"                # où décompresser dans Colab\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(\"Décompression terminée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb777308",
   "metadata": {},
   "source": [
    "## Création Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f31804",
   "metadata": {},
   "source": [
    "### Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25611231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiftiSegmentationDataset(Dataset):\n",
    "    \"\"\" NiftiSegmentationDataset for loading 3D NIfTI images \n",
    "        and their corresponding segmentation masks.\n",
    "\n",
    "    Args:\n",
    "        image_dir (str): Directory containing the NIfTI images.\n",
    "        mask_dir (str): Directory containing the segmentation masks.\n",
    "        transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        class_map (dict, optional): Mapping of class indices for remapping the segmentation masks.\n",
    "    \n",
    "    Returns:\n",
    "        image_slices (list): List of 2D image slices.\n",
    "        mask_slices (list): List of 2D mask slices.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, mask_dir, transform=False, class_map=None):\n",
    "        self.image_slices = []\n",
    "        self.mask_slices = []\n",
    "        self.transform = transform\n",
    "        self.class_map = class_map\n",
    "\n",
    "        image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.nii.gz')])\n",
    "        mask_files = set(os.listdir(mask_dir))\n",
    "\n",
    "        total_slices = 0\n",
    "        kept_slices = 0\n",
    "\n",
    "        for img_file in image_files:\n",
    "            index = os.path.splitext(os.path.splitext(img_file)[0])[0]\n",
    "            mask_file = f\"{index}.nii.gz\"\n",
    "            if mask_file in mask_files:\n",
    "                img_path = os.path.join(image_dir, img_file)\n",
    "                msk_path = os.path.join(mask_dir, mask_file)\n",
    "\n",
    "                img = nib.load(img_path).get_fdata()\n",
    "                msk = nib.load(msk_path).get_fdata()\n",
    "\n",
    "                assert img.shape == msk.shape, f\"Shape mismatch in {img_file}\"\n",
    "\n",
    "                for z in range(img.shape[-1]):\n",
    "                    slice_img = img[:, :, z]\n",
    "                    slice_msk = msk[:, :, z]\n",
    "\n",
    "                    total_slices += 1\n",
    "                    if np.any(slice_msk != 0):  # Ignore slices with only background\n",
    "                        self.image_slices.append(slice_img)\n",
    "                        self.mask_slices.append(slice_msk)\n",
    "                        kept_slices += 1\n",
    "\n",
    "        print(f\"Total slices scanned : {total_slices}\")\n",
    "        print(f\"Kept (non-empty) slices : {kept_slices}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_slices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.image_slices[idx]\n",
    "        mask = self.mask_slices[idx]\n",
    "\n",
    "        if self.class_map:\n",
    "            mask = remap_mask(mask, self.class_map)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image.astype(np.float32), mask=mask.astype(np.uint8))\n",
    "            image = augmented[\"image\"]\n",
    "            mask = augmented[\"mask\"].long()\n",
    "        else:\n",
    "            image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)\n",
    "            mask = torch.tensor(mask, dtype=torch.long)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "def generate_image_batch(data_batch: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Assemble a batch of image/mask pairs into batched tensors.\n",
    "\n",
    "    Args:\n",
    "        data_batch (list): List of (image, mask) tuples.\n",
    "            - image: Tensor of shape (C, H, W)\n",
    "            - mask : Tensor of shape (H, W)\n",
    "\n",
    "    Returns:\n",
    "        image_batch (Tensor): Batched images of shape (B, C, H, W)\n",
    "        mask_batch  (Tensor): Batched masks of shape (B, H, W)\n",
    "    \"\"\"\n",
    "    images, masks = zip(*data_batch)\n",
    "    image_batch = torch.stack(images)  # Shape: (B, C, H, W)\n",
    "    mask_batch = torch.stack(masks)    # Shape: (B, H, W)\n",
    "    return image_batch, mask_batch\n",
    "\n",
    "def remap_mask(mask: np.ndarray, mapping: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Remappe les valeurs des masques selon un dictionnaire de correspondance.\n",
    "\n",
    "    Args:\n",
    "        mask (np.ndarray): masque original\n",
    "        mapping (dict): ex. {0:0, 2:1, 3:2, 4:3, 5:4}\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: masque remappé avec classes consécutives\n",
    "    \"\"\"\n",
    "    remapped = np.zeros_like(mask, dtype=np.uint8)\n",
    "    for old_val, new_val in mapping.items():\n",
    "        remapped[mask == old_val] = new_val\n",
    "    return remapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33584058",
   "metadata": {},
   "source": [
    "#### Test de visualisation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af4eaf9",
   "metadata": {},
   "source": [
    "À ajuster selon l'utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = \"/content/dataset/dataset/mbh/nii/test/img/ID_0219ef88_ID_e5c1a31210.nii.gz\"\n",
    "# img = nib.load(img_path)\n",
    "# data = img.get_fdata()\n",
    "\n",
    "# print(\"Shape :\", data.shape)              # (H, W, D) ? (D, H, W) ?\n",
    "# print(\"Data type :\", data.dtype)\n",
    "# print(\"Min/Max :\", data.min(), data.max())\n",
    "\n",
    "# print(f\"Nombre de dimensions : {data.ndim}\")\n",
    "# for i, dim in enumerate(data.shape):\n",
    "#     print(f\"Dim {i} size = {dim}\")\n",
    "\n",
    "\n",
    "# plt.imshow(data[:, :, data.shape[2] // 2], cmap=\"gray\")\n",
    "# plt.title(\"Coupe axiale (tranche centrale)\")\n",
    "# plt.show()\n",
    "\n",
    "# img_path = \"/content/dataset/dataset/mbh/nii/test/seg/ID_0219ef88_ID_e5c1a31210.nii.gz\"\n",
    "# img = nib.load(img_path)\n",
    "# data = img.get_fdata()\n",
    "\n",
    "# print(\"Shape :\", data.shape)              # (H, W, D) ? (D, H, W) ?\n",
    "# print(\"Data type :\", data.dtype)\n",
    "# print(\"Min/Max :\", data.min(), data.max())\n",
    "\n",
    "# print(f\"Nombre de dimensions : {data.ndim}\")\n",
    "# for i, dim in enumerate(data.shape):\n",
    "#     print(f\"Dim {i} size = {dim}\")\n",
    "\n",
    "\n",
    "# plt.imshow(data[:, :, data.shape[2] // 2], cmap=\"gray\")\n",
    "# plt.title(\"Coupe axiale (tranche centrale)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e447dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3709b5c4",
   "metadata": {},
   "source": [
    "### Initialisation des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b385cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Initilisation des datasets:\n",
    "###########################\n",
    "\n",
    "base_path_nii = \"/content/dataset/dataset/mbh/nii\"\n",
    "\n",
    "# TRAIN\n",
    "train_image_dir = os.path.join(base_path_nii, \"train\", \"img\")\n",
    "train_mask_dir  = os.path.join(base_path_nii, \"train\", \"seg\")\n",
    "\n",
    "# VAL\n",
    "val_image_dir = os.path.join(base_path_nii, \"val\", \"img\")\n",
    "val_mask_dir  = os.path.join(base_path_nii, \"val\", \"seg\")\n",
    "\n",
    "# TEST\n",
    "test_image_dir = os.path.join(base_path_nii, \"test\", \"img\")\n",
    "test_mask_dir  = os.path.join(base_path_nii, \"test\", \"seg\")\n",
    "\n",
    "# Sans augmentation:\n",
    "train_dataset = NiftiSegmentationDataset(train_image_dir, train_mask_dir)\n",
    "val_dataset   = NiftiSegmentationDataset(val_image_dir, val_mask_dir)\n",
    "test_dataset  = NiftiSegmentationDataset(test_image_dir, test_mask_dir)\n",
    "\n",
    "### Pour tester les datasets:\n",
    "# idx = 2  # change l'index si nécessaire\n",
    "# image, mask = test_dataset[idx]  # train_dataset est une instance de NiftiSegmentationDataset\n",
    "\n",
    "# # Affichage d'une image et de son masque\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.suptitle(f\"Tranche {idx} - Retenue\")\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.title(\"Image\")\n",
    "# plt.imshow(image.squeeze().numpy(), cmap=\"gray\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.title(\"Masque\")\n",
    "# plt.imshow(mask.numpy(), cmap=\"jet\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "## Initialistion de la variable d'augmentation:\n",
    "train_transform = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"seg\"]),\n",
    "    RandFlipd(keys=[\"image\", \"seg\"], prob=0.5, spatial_axis=0),  # Horizontal (X)\n",
    "    RandFlipd(keys=[\"image\", \"seg\"], prob=0.5, spatial_axis=1),  # Vertical (Y)\n",
    "    RandRotate90d(keys=[\"image\", \"seg\"], prob=0.5, max_k=3),     # 90° rotation\n",
    "    RandAffined(\n",
    "        keys=[\"image\", \"seg\"],\n",
    "        prob=0.5,\n",
    "        rotate_range=(0.2, 0.2, 0.2),  # approx ±25°\n",
    "        shear_range=(0.1, 0.1, 0.1),\n",
    "        padding_mode='border'\n",
    "    ),\n",
    "    RandGridDistortiond(\n",
    "        keys=[\"image\", \"seg\"],\n",
    "        prob=0.3,\n",
    "        distort_limit=0.3\n",
    "    ),\n",
    "    RandAdjustContrastd(keys=[\"image\"], prob=0.3, gamma=(0.8, 1.2)),\n",
    "    NormalizeIntensityd(keys=[\"image\"], nonzero=False, channel_wise=True),\n",
    "    ToTensord(keys=[\"image\", \"seg\"])\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"seg\"]),\n",
    "    NormalizeIntensityd(keys=[\"image\"], nonzero=False, channel_wise=True),\n",
    "    ToTensord(keys=[\"image\", \"seg\"])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d976ab10",
   "metadata": {},
   "source": [
    "## Architecture U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Double convolution block for U-Net.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "\n",
    "    Returns:\n",
    "        x (Tensor): Output tensor after double convolution.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Classic U-Net architecture for image segmentation.\n",
    "\n",
    "    Args:\n",
    "        n_channels (int): Number of input channels (e.g., 1 for grayscale).\n",
    "        n_classes (int): Number of output classes for segmentation.\n",
    "    \n",
    "    Returns:\n",
    "        logits (Tensor): Output tensor of shape (B, n_classes, H, W).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super().__init__()\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = self.down_block(64, 128)\n",
    "        self.down2 = self.down_block(128, 256)\n",
    "        self.down3 = self.down_block(256, 512)\n",
    "        self.down4 = self.down_block(512, 1024)\n",
    "\n",
    "        self.up1 = self.up_block(1024, 512)\n",
    "        self.up2 = self.up_block(512, 256)\n",
    "        self.up3 = self.up_block(256, 128)\n",
    "        self.up4 = self.up_block(128, 64)\n",
    "\n",
    "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def down_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def up_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        x = self.crop_and_concat(self.up1[0](x5), x4)\n",
    "        x = self.up1[1](x)\n",
    "\n",
    "        x = self.crop_and_concat(self.up2[0](x), x3)\n",
    "        x = self.up2[1](x)\n",
    "\n",
    "        x = self.crop_and_concat(self.up3[0](x), x2)\n",
    "        x = self.up3[1](x)\n",
    "\n",
    "        x = self.crop_and_concat(self.up4[0](x), x1)\n",
    "        x = self.up4[1](x)\n",
    "\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "    def crop_and_concat(self, upsampled, bypass):\n",
    "        _, _, H, W = upsampled.shape\n",
    "        bypass_cropped = self.center_crop(bypass, H, W)\n",
    "        return torch.cat([bypass_cropped, upsampled], dim=1)\n",
    "\n",
    "    def center_crop(self, tensor, target_h, target_w):\n",
    "        _, _, h, w = tensor.shape\n",
    "        start_x = (w - target_w) // 2\n",
    "        start_y = (h - target_h) // 2\n",
    "        return tensor[:, :, start_y:start_y + target_h, start_x:start_x + target_w]\n",
    "\n",
    "def count_unique_labels(dataset):\n",
    "    \"\"\"\n",
    "    Count unique labels in the dataset for testing.\n",
    "    Args:\n",
    "        dataset (Dataset): PyTorch dataset containing images and masks.\n",
    "    Returns:\n",
    "        list: Sorted list of unique labels.\n",
    "    \"\"\"\n",
    "    unique_labels = set()\n",
    "    for i in range(len(dataset)):\n",
    "        _, mask = dataset[i]\n",
    "        unique_labels.update(torch.unique(mask).tolist())\n",
    "    return sorted(unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0ae6b",
   "metadata": {},
   "source": [
    "## Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0f93cf",
   "metadata": {},
   "source": [
    "### Fonctions de pertes et weighted maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weight_map(mask, w0=20, sigma=3):\n",
    "    \"\"\"\n",
    "    Compute pixel-wise weight map for segmentation training.\n",
    "\n",
    "    Args:\n",
    "        mask (np.array): Labeled 2D mask.\n",
    "        w0 (float): Weight for separation term.\n",
    "        sigma (float): Std-dev for border emphasis.\n",
    "\n",
    "    Returns:\n",
    "        np.array: weight map w(x)\n",
    "    \"\"\"\n",
    "    labeled_mask = label(mask)\n",
    "    num_objects = np.max(labeled_mask)\n",
    "\n",
    "    if num_objects < 2:\n",
    "        return np.ones_like(mask, dtype=np.float32)\n",
    "\n",
    "    # Distances aux autres objets\n",
    "    dists = np.zeros((num_objects, *mask.shape), dtype=np.float32)\n",
    "    for i in range(1, num_objects + 1):\n",
    "        dists[i - 1] = distance_transform_edt(labeled_mask != i)\n",
    "\n",
    "    d1 = np.min(dists, axis=0)\n",
    "    dists[dists == d1] = np.inf\n",
    "    d2 = np.min(dists, axis=0)\n",
    "    separation = w0 * np.exp(-((d1 + d2) ** 2) / (2 * sigma ** 2))\n",
    "\n",
    "    # Pondération par fréquence de classe\n",
    "    class_weights = np.ones_like(mask, dtype=np.float32)\n",
    "    labels, counts = np.unique(mask, return_counts=True)\n",
    "    total = np.sum(counts)\n",
    "    freqs = {val: count / total for val, count in zip(labels, counts)}\n",
    "\n",
    "    for val in labels:\n",
    "        # if val == 0:\n",
    "        #     continue\n",
    "        class_weights[mask == val] = 1.0 / (freqs[val] + 1e-6)\n",
    "\n",
    "    # Normaliser pour stabiliser:\n",
    "    class_weights /= np.mean(class_weights)\n",
    "\n",
    "    return class_weights + separation\n",
    "\n",
    "def compute_weight_batch(batch_masks):\n",
    "    \"\"\"\n",
    "    Compute the weight map for each mask in a batch.\n",
    "    Returns a tensor of shape (B, H, W)\n",
    "    \n",
    "    Args:\n",
    "        batch_masks (Tensor): Shape (B, H, W) — batch of masks\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: Shape (B, H, W) — batch of weight maps\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    for mask in batch_masks:\n",
    "        mask_np = mask.cpu().numpy()\n",
    "        mask_np = (mask_np > 0).astype(np.uint8)  # binarize the mask\n",
    "        w = compute_weight_map(mask_np)\n",
    "        weights.append(torch.tensor(w, dtype=torch.float32))\n",
    "    return torch.stack(weights).to(device)\n",
    "\n",
    "def weighted_cross_entropy_loss(logits, targets, weight_map):\n",
    "    \"\"\"\n",
    "    Computes the pixel-wise weighted cross-entropy loss as defined in Eq. (1) of the U-Net paper.\n",
    "\n",
    "    Args:\n",
    "        logits (Tensor): Network output, shape (B, C, H, W), unnormalized scores for each class.\n",
    "        targets (Tensor): Ground truth segmentation, shape (B, H, W), with class indices.\n",
    "        weight_map (Tensor): Weight map w(x), shape (B, H, W), computed from the ground truth mask.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Scalar loss value.\n",
    "    \"\"\"\n",
    "    assert logits.device == targets.device == weight_map.device\n",
    "    assert logits.dtype in [torch.float32, torch.float16, torch.bfloat16]\n",
    "    assert weight_map.dtype in [torch.float32, torch.float16, torch.bfloat16]\n",
    "\n",
    "    B, C, H, W = logits.shape\n",
    "\n",
    "    # Compute log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "\n",
    "    # Select log-probabilities of the correct class at each pixel\n",
    "    log_probs_target = log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Apply pixel-wise weights\n",
    "    loss = -weight_map * log_probs_target\n",
    "    return loss.mean()\n",
    "\n",
    "def dice_loss(logits: torch.Tensor, targets: torch.Tensor, smooth: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute class-weighted multi-class Dice Loss between logits and ground truth labels.\n",
    "\n",
    "    Args:\n",
    "        logits (Tensor): Shape (B, C, H, W) — raw output from the model (before softmax)\n",
    "        targets (Tensor): Shape (B, H, W) — ground truth class indices\n",
    "        smooth (float): smoothing constant to avoid division by zero\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Dice loss value (scalar)\n",
    "    \"\"\"\n",
    "    num_classes = logits.shape[1]\n",
    "    probs = F.softmax(logits, dim=1)                           # (B, C, H, W)\n",
    "    targets_one_hot = F.one_hot(targets, num_classes)          # (B, H, W, C)\n",
    "    targets_one_hot = targets_one_hot.permute(0, 3, 1, 2).float()  # (B, C, H, W)\n",
    "\n",
    "    intersection = (probs * targets_one_hot).sum(dim=(0, 2, 3))\n",
    "    union = probs.sum(dim=(0, 2, 3)) + targets_one_hot.sum(dim=(0, 2, 3))\n",
    "\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "    # Calcul dynamique de la fréquence de pixels par classe\n",
    "    class_pixel_counts = targets_one_hot.sum(dim=(0, 2, 3))  # (C,)\n",
    "    class_weights = 1.0 / (class_pixel_counts + 1e-6)\n",
    "    class_weights = torch.clamp(class_weights, max=10.0)\n",
    "    class_weights /= class_weights.sum()\n",
    "\n",
    "    weighted_dice_loss = (1.0 - dice) * class_weights\n",
    "    return weighted_dice_loss.sum()\n",
    "\n",
    "def combined_loss(logits: torch.Tensor, targets: torch.Tensor, weight_map: torch.Tensor, smooth: float = 1.0, w_ce: float = 0.3, w_dice: float = 0.7) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Combines the Weighted Cross-Entropy Loss and Dice Loss.\n",
    "\n",
    "    Args:\n",
    "        logits (Tensor): Shape (B, C, H, W) — raw output from the model (before softmax)\n",
    "        targets (Tensor): Shape (B, H, W) — ground truth class indices\n",
    "        weight_map (Tensor): Shape (B, H, W) — pixel-wise weight map\n",
    "        smooth (float): Smoothing constant for Dice Loss to avoid division by zero\n",
    "        w_ce (float): Weight for the Cross-Entropy loss component\n",
    "        w_dice (float): Weight for the Dice loss component\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Combined loss value\n",
    "    \"\"\"\n",
    "    # Compute Dice Loss\n",
    "    num_classes = logits.shape[1]\n",
    "    probs = F.softmax(logits, dim=1)                    # (B, C, H, W)\n",
    "    targets_one_hot = F.one_hot(targets, num_classes)   # (B, H, W, C)\n",
    "    targets_one_hot = targets_one_hot.permute(0, 3, 1, 2).float()  # (B, C, H, W)\n",
    "\n",
    "    intersection = (probs * targets_one_hot).sum(dim=(0, 2, 3))\n",
    "    union = probs.sum(dim=(0, 2, 3)) + targets_one_hot.sum(dim=(0, 2, 3))\n",
    "\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    dice_loss = 1.0 - dice.mean()\n",
    "\n",
    "    # Compute Weighted Cross-Entropy Loss\n",
    "    assert logits.device == targets.device == weight_map.device\n",
    "    assert logits.dtype in [torch.float32, torch.float16, torch.bfloat16]\n",
    "    assert weight_map.dtype in [torch.float32, torch.float16, torch.bfloat16]\n",
    "\n",
    "    B, C, H, W = logits.shape\n",
    "\n",
    "    # Compute log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "\n",
    "    # Select log-probabilities of the correct class at each pixel\n",
    "    log_probs_target = log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Apply pixel-wise weights\n",
    "    ce_loss = -weight_map * log_probs_target\n",
    "\n",
    "    # Combine both losses\n",
    "    total_loss = (w_ce * ce_loss.mean()) + (w_dice * dice_loss)\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def loss_batch_unet(model: nn.Module, images: torch.Tensor, masks: torch.Tensor, config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Compute loss and metrics for a U-Net batch (segmentation).\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): U-Net model\n",
    "        images (Tensor): shape (B, C, H, W)\n",
    "        masks (Tensor): shape (B, H, W) with integer class labels\n",
    "        config (dict): contains:\n",
    "            - 'device': torch.device\n",
    "            - 'loss': loss function\n",
    "            - 'weight_map_fn': function that generates weight maps from masks\n",
    "\n",
    "    Returns:\n",
    "        dict: {'loss': ..., 'iou': ..., 'accuracy': ...}\n",
    "    \"\"\"\n",
    "    device = config['device']\n",
    "    loss_fn = config['loss']\n",
    "    weight_map_fn = config.get('weight_map_fn', None)\n",
    "\n",
    "    images = images.to(device)\n",
    "    masks = masks.to(device)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)  # shape: (B, C, H, W)\n",
    "\n",
    "        if weight_map_fn:\n",
    "            weight_map = weight_map_fn(masks)\n",
    "            loss = loss_fn(outputs, masks, weight_map)\n",
    "        else:\n",
    "            loss = loss_fn(outputs, masks)\n",
    "\n",
    "        # Get predicted classes\n",
    "        preds = torch.argmax(outputs, dim=1)  # (B, H, W)\n",
    "\n",
    "        # IoU\n",
    "        intersection = torch.logical_and(preds == 1, masks == 1).sum(dim=(1, 2)).float()\n",
    "        union = torch.logical_or(preds == 1, masks == 1).sum(dim=(1, 2)).float()\n",
    "        iou = (intersection / (union + 1e-6)).mean().item()\n",
    "\n",
    "        # Pixel accuracy\n",
    "        correct = (preds == masks).float().mean().item()\n",
    "\n",
    "    return {'loss': loss.item(), 'iou': iou, 'accuracy': correct}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e6ab8",
   "metadata": {},
   "source": [
    "### Fonctions d'évaluation de performances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model: nn.Module, dataloader: DataLoader, config: dict, topk_list=[1, 3, 5]) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate U-Net on a dataset and compute multiple metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained segmentation model.\n",
    "        dataloader (DataLoader): Validation or test loader.\n",
    "        config (dict): Must contain 'device', 'loss', and optionally 'weight_map_fn'.\n",
    "        topk_list (list): List of top-k accuracies to compute.\n",
    "\n",
    "    Returns:\n",
    "        dict: Averaged metrics over the dataset: loss, accuracy, mean IoU, top-k.\n",
    "    \"\"\"\n",
    "    device = config[\"device\"]\n",
    "    loss_fn = config[\"loss_fn\"]\n",
    "    weight_map_fn = config.get(\"weight_map_fn\", None)\n",
    "    logs = defaultdict(list)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Loss fonction:\n",
    "            if weight_map_fn:\n",
    "                weight_map = weight_map_fn(masks).to(device)\n",
    "                loss = loss_fn(outputs, masks, weight_map)\n",
    "            else:\n",
    "                loss = loss_fn(outputs, masks)\n",
    "\n",
    "            logs[\"loss\"].append(loss.item())\n",
    "\n",
    "            # Pixel accuracy\n",
    "            correct = (preds == masks).float().mean().item()\n",
    "            logs[\"accuracy\"].append(correct)\n",
    "\n",
    "            # Per-class IoU (excluding background = 0)\n",
    "            ious = []\n",
    "            num_classes = outputs.shape[1]\n",
    "            for cls in range(1, num_classes):  # ignore background\n",
    "                pred_cls = (preds == cls)\n",
    "                mask_cls = (masks == cls)\n",
    "                intersection = torch.logical_and(pred_cls, mask_cls).sum().float()\n",
    "                union = torch.logical_or(pred_cls, mask_cls).sum().float()\n",
    "                if union > 0:\n",
    "                    iou = intersection / (union + 1e-6)\n",
    "                    ious.append(iou.item())\n",
    "\n",
    "            mean_iou = np.mean(ious) if ious else 0.0\n",
    "            logs[\"IoU\"].append(mean_iou)\n",
    "\n",
    "            # Top-k pixel accuracy\n",
    "            for k in topk_list:\n",
    "                acc_k = topk_pixel_accuracy(outputs, masks, k)\n",
    "                logs[f\"top-{k}\"].append(acc_k)\n",
    "\n",
    "    return {k: np.mean(v) for k, v in logs.items()}\n",
    "\n",
    "def topk_pixel_accuracy(logits: torch.Tensor, targets: torch.Tensor, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute pixel-wise top-k accuracy for segmentation.\n",
    "\n",
    "    Args:\n",
    "        logits (Tensor): shape (B, C, H, W)\n",
    "        targets (Tensor): shape (B, H, W)\n",
    "        k (int): top-k value\n",
    "\n",
    "    Returns:\n",
    "        float: top-k pixel accuracy\n",
    "    \"\"\"\n",
    "    B, C, H, W = logits.shape\n",
    "    targets_flat = targets.view(B, -1)\n",
    "    logits_flat = logits.view(B, C, -1).transpose(1, 2)  # (B, H*W, C)\n",
    "\n",
    "    topk = logits_flat.topk(k, dim=-1).indices           # (B, H*W, k)\n",
    "    targets_exp = targets_flat.unsqueeze(-1).expand_as(topk)  # (B, H*W, k)\n",
    "\n",
    "    correct = (topk == targets_exp).any(dim=-1).float()\n",
    "    return correct.mean().item()\n",
    "\n",
    "def show_predictions(model, dataset, device, n=3):\n",
    "    \"\"\"\n",
    "    Visualize `n` predictions from the dataset using the trained model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained U-Net model\n",
    "        dataset (Dataset): PyTorch dataset (ex: val_dataset)\n",
    "        device (torch.device): \"cuda\" or \"cpu\"\n",
    "        n (int): Number of samples to display\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    n = min(n, len(dataset))  # ne pas dépasser la taille du dataset\n",
    "\n",
    "    plt.figure(figsize=(12, 4 * n))\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            image, true_mask = dataset[i]\n",
    "            image = image.to(device).unsqueeze(0)  # (1, 1, H, W)\n",
    "            pred = model(image)  # (1, C, H, W)\n",
    "            pred_mask = torch.argmax(pred.squeeze(), dim=0).cpu()\n",
    "\n",
    "            plt.subplot(n, 3, i * 3 + 1)\n",
    "            plt.title(\"Input Image\")\n",
    "            plt.imshow(image.squeeze().cpu().numpy(), cmap=\"gray\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "def print_logs(dataset_type: str, logs: dict):\n",
    "    \"\"\"\n",
    "    Nicely format and print log metrics for a given dataset type.\n",
    "\n",
    "    Args:\n",
    "        dataset_type (str): \"Train\", \"Val\", or \"Test\"\n",
    "        logs (dict): Dictionary of metric names and values (floats)\n",
    "    \"\"\"\n",
    "    formatted = [\n",
    "        f\"{name}: {value:.8f}\"\n",
    "        for name, value in logs.items()\n",
    "    ]\n",
    "    desc = f\"{dataset_type} —\\t\" + \"\\t\".join(formatted)\n",
    "    print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afedadb",
   "metadata": {},
   "source": [
    "### Training Block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9fefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, val_loader, optimizer, config, n_epochs, log_wandb=True, \n",
    "                       show_preds=None, dataset_for_display=None, n_logged_images=3, save_path=\"Latest_best_model.pth\"):\n",
    "    \"\"\"\n",
    "    Train and validate the model for a specified number of epochs.  \n",
    "    \n",
    "    Args:   \n",
    "        model (nn.Module): U-Net model\n",
    "        train_loader (DataLoader): Training data loader\n",
    "        val_loader (DataLoader): Validation data loader\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training\n",
    "        config (dict): Configuration dictionary containing:\n",
    "            - 'device': torch.device\n",
    "            - 'loss_fn': loss function\n",
    "            - 'weight_map_fn': function to compute weight maps (optional)\n",
    "            - 'scheduler': learning rate scheduler (optional)\n",
    "        n_epochs (int): Number of epochs to train\n",
    "        log_wandb (bool): Whether to log metrics to Weights & Biases\n",
    "        show_preds (bool): Whether to show predictions during training\n",
    "        dataset_for_display (Dataset): Dataset to use for displaying predictions\n",
    "        n_logged_images (int): Number of images to log\n",
    "        save_path (str): Path to save the best model\n",
    "    \"\"\"\n",
    "    device = config[\"device\"]\n",
    "    loss_fn = config[\"loss_fn\"]\n",
    "    weight_map_fn = config.get(\"weight_map_fn\", None)\n",
    "    scheduler = config.get(\"scheduler\", None)  # ajout pour compatibilité scheduler\n",
    "    best_val_iou = 0.0\n",
    "    num_used_batches = 0\n",
    "\n",
    "    if log_wandb:\n",
    "        log_table = wandb.Table(columns=[\"epoch\", \"image\", \"ground_truth\", \"prediction\"])\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{n_epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for images, masks in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            if torch.all(masks == 0):\n",
    "                # print(\" Batch sans aucune classe foreground — ignoré\")\n",
    "                continue\n",
    "            num_used_batches += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            weight_map = weight_map_fn(masks).to(device) if weight_map_fn else None\n",
    "            outputs = model(images)\n",
    "\n",
    "            assert masks.max() < outputs.shape[1], \\\n",
    "                f\"Target max label {masks.max().item()} >= n_classes={outputs.shape[1]}\"\n",
    "            loss = loss_fn(outputs, masks, weight_map) if weight_map is not None else loss_fn(outputs, masks)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / num_used_batches\n",
    "        print(f\"\\nTraining Loss: {avg_train_loss:.8f}\")\n",
    "\n",
    "        # Validation\n",
    "        val_logs = eval_model(model, val_loader, config, topk_list=[1, 3, 5])\n",
    "        print_logs(\"Validation\", val_logs)\n",
    "\n",
    "        val_iou = val_logs.get(\"IoU\", 0.0)\n",
    "        if val_iou > best_val_iou:\n",
    "            print(f\"New best IoU: {val_iou:.8f} (previous: {best_val_iou:.8f}) — saving model.\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            best_val_iou = val_iou\n",
    "\n",
    "        if log_wandb and (epoch + 1) % 2 == 0:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                **{f\"val_{k}\": v for k, v in val_logs.items()}\n",
    "            })\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()  # mise à jour du learning rate\n",
    "\n",
    "        if show_preds and dataset_for_display is not None and (epoch) % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for i in range(min(n_logged_images, len(dataset_for_display))):\n",
    "                    if i % 75 != 0:\n",
    "                        continue\n",
    "                    img, mask = dataset_for_display[i]\n",
    "                    input_tensor = img.unsqueeze(0).to(device)\n",
    "                    pred = model(input_tensor)\n",
    "                    pred_mask = torch.argmax(pred.squeeze(), dim=0).cpu()\n",
    "\n",
    "                    if log_wandb:\n",
    "                        wandb.log({\n",
    "                            f\"Example/{i}/image\": wandb.Image(img.squeeze().cpu().numpy(), caption=\"Input\"),\n",
    "                            f\"Example/{i}/mask\": wandb.Image(mask.cpu().numpy(), caption=\"Ground Truth\"),\n",
    "                            f\"Example/{i}/prediction\": wandb.Image(pred_mask.numpy(), caption=\"Prediction\")\n",
    "                        })\n",
    "                        log_table.add_data(\n",
    "                            epoch + 1,\n",
    "                            wandb.Image(img.squeeze().cpu().numpy(), caption=\"Input\"),\n",
    "                            wandb.Image(mask.cpu().numpy(), caption=\"Ground Truth\"),\n",
    "                            wandb.Image(pred_mask.numpy(), caption=\"Prediction\")\n",
    "                        )\n",
    "\n",
    "    if log_wandb:\n",
    "        wandb.log({\"Predictions Table\": log_table})\n",
    "        print(\"Final wandb table logged.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e2e59",
   "metadata": {},
   "source": [
    "## Boucle d'entraînement et de compilations des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2cf06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Training and evaluation loop:\n",
    "######################################################\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "batch_sizes = [16]\n",
    "epoch_values = [500]\n",
    "learning_rates = [1e-4]\n",
    "n_classes = 6\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(lr)\n",
    "    for epochs in epoch_values:\n",
    "        torch.cuda.empty_cache()\n",
    "        print(epochs)\n",
    "        print()\n",
    "        for batch_size in batch_sizes:\n",
    "            model = UNet(n_channels=1, n_classes=n_classes)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            # Cosine Decay\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "            ### Version dataset 3D:\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=4,\n",
    "                pin_memory=True,\n",
    "                drop_last=True\n",
    "            )\n",
    "\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=4,\n",
    "                pin_memory=True,\n",
    "                drop_last=False\n",
    "            )\n",
    "\n",
    "##### Configurations pour le training avec différentes fonctions de perte:\n",
    "### 1) Weighted Cross Entropy Loss\n",
    "            config = {\n",
    "                      \"architecture\": \"U-Net\",\n",
    "                      \"loss\": \"Weighted Cross Entropy\",\n",
    "                      \"epochs\": epochs,\n",
    "                      \"batch_size\": batch_size,\n",
    "                      \"optimizer\": \"Adam\",\n",
    "                      \"lr\": lr,\n",
    "                      \"n_classes\": n_classes,\n",
    "                      \"dataset\": base_path_nii,\n",
    "                      \"device\": device,\n",
    "                      \"loss_fn\": weighted_cross_entropy_loss,\n",
    "                      \"train_loader\": train_loader,\n",
    "                      \"val_loader\": val_loader,\n",
    "                      \"optimizer_instance\": optimizer,\n",
    "                      \"scheduler\": scheduler,\n",
    "                      \"weight_map_fn\": compute_weight_batch\n",
    "                  }\n",
    "\n",
    "\n",
    "### 2) Dice Loss\n",
    "            # config = {\n",
    "            #           \"architecture\": \"U-Net\",\n",
    "            #           \"loss\": \"Dice Loss\",\n",
    "            #           \"epochs\": epochs,\n",
    "            #           \"batch_size\": batch_size,\n",
    "            #           \"optimizer\": \"Adam\",\n",
    "            #           \"lr\": lr,\n",
    "            #           \"n_classes\": n_classes,\n",
    "            #           \"dataset\": base_path_nii,\n",
    "            #           \"device\": device,\n",
    "            #           \"loss_fn\": dice_loss,\n",
    "            #           \"train_loader\": train_loader,\n",
    "            #           \"val_loader\": val_loader,\n",
    "            #           \"optimizer_instance\": optimizer,\n",
    "            #           \"scheduler\": scheduler,\n",
    "            #           \"weight_map_fn\": None  # pas avec DiceLoss\n",
    "            #       }\n",
    "\n",
    "### 3) Combined Loss (Weighted Cross Entropy + Dice Loss)\n",
    "            # config = {\n",
    "            #           \"architecture\": \"U-Net\",\n",
    "            #           \"loss\": \"combined_loss\",\n",
    "            #           \"epochs\": epochs,\n",
    "            #           \"batch_size\": batch_size,\n",
    "            #           \"optimizer\": \"Adam\",\n",
    "            #           \"lr\": lr,\n",
    "            #           \"n_classes\": n_classes,\n",
    "            #           \"dataset\": base_path_nii,\n",
    "            #           \"device\": device,\n",
    "            #           \"loss_fn\": combined_loss,\n",
    "            #           \"train_loader\": train_loader,\n",
    "            #           \"val_loader\": val_loader,\n",
    "            #           \"optimizer_instance\": optimizer,\n",
    "            #           \"scheduler\": scheduler,\n",
    "            #           \"weight_map_fn\": compute_weight_batch\n",
    "            #       }\n",
    "\n",
    "            model.to(config[\"device\"])\n",
    "            run_name = f\"U-Net_3DCE_1\"\n",
    "\n",
    "#### Compilation des logs pour wandb:\n",
    "            with wandb.init(\n",
    "                config=config,\n",
    "                project=f\"Projet U-Net - LR = 1e-03\",\n",
    "                group=\"U-Net Cell Segmentation\",\n",
    "                name=run_name,\n",
    "                save_code=True,\n",
    "            ):\n",
    "                train_and_validate(\n",
    "                    model=model,\n",
    "                    train_loader=config[\"train_loader\"],\n",
    "                    val_loader=config[\"val_loader\"],\n",
    "                    optimizer=config[\"optimizer_instance\"],\n",
    "                    config={\n",
    "                        \"device\": config[\"device\"],\n",
    "                        \"loss_fn\": config[\"loss_fn\"],\n",
    "                        \"weight_map_fn\": config[\"weight_map_fn\"],\n",
    "                        \"scheduler\": config[\"scheduler\"]\n",
    "                    },\n",
    "                    n_epochs=config[\"epochs\"],\n",
    "                    log_wandb=True,\n",
    "                    show_preds=True,\n",
    "                    dataset_for_display=val_dataset\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
